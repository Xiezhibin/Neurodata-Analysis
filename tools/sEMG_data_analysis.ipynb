{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size=5>sEMG Data Analysis</font></center>**\n",
    "***<center>Calculation of sEMG Atomic Metrics</center>***\n",
    "***\n",
    "**author**: Daniel Tse\n",
    "\n",
    "**date**: 27th June, 2024\n",
    "\n",
    "**[GitHub Repository](https://github.com/Xiezhibin/Neurodata-Analysis)**\n",
    "\n",
    "#### Table of Contents\n",
    "- <a href='#intro'>1. Project Overview</a> \n",
    "- <a href='#pre'>2. Data Preprocessing</a>\n",
    "- <a href='#sample'>3. sEMG Data Analysis</a> \n",
    "- <a href='#s1'>3.1. Time-field analysis</a>\n",
    "- <a href='#s2m'>3.2.Frequency-field analysis</a>\n",
    "- <a href='#s2nm'>3.3. Fuzzy entropy</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='intro'>1. Project Overview</a>\n",
    "\n",
    "The primary goal of this analysis is to investigate how neural and muscular activities are related and how these relationships can be used to improve rehabilitation outcomes for stroke survivors. The sEMG data measures the electrical activity produced by skeletal muscles, while NIRS data provides insights into the neural activity by measuring blood flow and oxygenation in the brain.\n",
    "\n",
    "We aim to determine if there are significant correlations between the neural and muscular activities and to identify which specific muscles and brain regions are involved. This understanding can potentially lead to more effective rehabilitation protocols and improved quality of life for stroke survivors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='pre'>2. Data Preprocessing</a>\n",
    "\n",
    "In this study, raw sEMG signals from BIC and TRI underwent initial filtering with a fourth-order bandpass Butterworth filter (20–450 Hz) to remove motion artifacts, low-frequency drift, and high-frequency noise. Subsequently, a notch filter (adjustable bandwidth, centered at 50 Hz) was applied to mitigate power line interference and harmonics from environmental sources. The processed data was then smoothed and normalized to facilitate comparison. To ensure data stability post-exercise onset acceleration, the initial 20 seconds were excluded, retaining a stable 40-second signal segment for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import EntropyHub as EH\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"xxx.txt\"  # 请替换为你的文件路径\n",
    "\n",
    "# 打开文件\n",
    "with open(\"../\" + file_path, \"r\") as file:\n",
    "    # 跳过前10行\n",
    "    for _ in range(10):\n",
    "        next(file)\n",
    "    \n",
    "    # 读取剩余的内容\n",
    "    remaining_content = file.read()\n",
    "\n",
    "    # 使用 StringIO 将字符串内容转为文件对象\n",
    "    file_like_object = StringIO(remaining_content)\n",
    "\n",
    "    # 读取数据到 Pandas DataFrame\n",
    "    df = pd.read_csv(file_like_object, delimiter=r\"\\s+\")\n",
    "\n",
    "# 打印剩余的内容\n",
    "df.set_index('Frame', inplace=True)\n",
    "\n",
    "\n",
    "column = df.columns[1]\n",
    "\n",
    "\n",
    "# 找到NaN值所在的行索引\n",
    "nan_indices = df.index[df[column].isna()].tolist()\n",
    "\n",
    "# 检查是否有足够的NaN值来拆分成五段\n",
    "if len(nan_indices) < 6:\n",
    "    print(\"NaN值的数量不足以拆分成五段。\")\n",
    "else:\n",
    "    # 创建一个用于存储拆分后的DataFrame的列表\n",
    "    split_dfs = []\n",
    "\n",
    "    # 初始化拆分段的起始索引\n",
    "    start_index = 0\n",
    "\n",
    "    # 拆分文件成五段\n",
    "    for i in range(5):\n",
    "        # 寻找下一个连续的NaN值段\n",
    "        while start_index < len(nan_indices) - 1 and nan_indices[start_index + 1] - nan_indices[start_index] == 1:\n",
    "            start_index += 1\n",
    "\n",
    "        # 计算当前段的起始和结束索引\n",
    "        act_start = nan_indices[start_index] + 1\n",
    "        if start_index < len(nan_indices) - 1:\n",
    "            act_end = nan_indices[start_index + 1] - 1\n",
    "        else:\n",
    "            act_end = len(df) - 1\n",
    "\n",
    "        print(f\"第 {i + 1} 段起始索引：{act_start}，结束索引：{act_end}\")\n",
    "\n",
    "        # 切片并将段添加到拆分后的DataFrame列表中\n",
    "        split_df = df.iloc[act_start:act_end + 1]\n",
    "        split_dfs.append(split_df)\n",
    "\n",
    "        # 移动起始索引到下一个不连续的NaN值段\n",
    "        start_index += 1\n",
    "\n",
    "# 打印每个拆分段的数据框（示例）\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    print(f\"拆分段 {i + 1}:\\n{split_df}\")\n",
    "\n",
    "\n",
    "# 创建一个用于存储拆分后的DataFrame的列表\n",
    "split_rms = []\n",
    "\n",
    "# 计算均方根\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    rms = math.sqrt(sum(x ** 2 for x in split_df[column]) / len(split_df))\n",
    "    split_rms.append(rms)\n",
    "\n",
    "\n",
    "\n",
    "# 使用 MinMaxScaler 进行最小-最大归一化\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# 创建一个用于存储拆分后的DataFrame的列表\n",
    "split_normal_dfs = []\n",
    "\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    split_normal_dfs.append(scaler_minmax.fit_transform(pd.DataFrame(split_df[column])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, split_df in enumerate(split_normal_dfs):\n",
    "\n",
    "    # 创建时间序列图\n",
    "    plt.figure(figsize=(10, 6))  # 可以调整图形大小\n",
    "    plt.plot(split_df, marker='o', linestyle='-')  # 使用圆点连接线\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(column)\n",
    "    plt.title(f\"{i}th Time Series\")\n",
    "    plt.grid(True)  # 添加网格线\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='sample'>3. sEMG Data Analysis</a> \n",
    "# <a id='s1'>3.1. Time-field analysis</a>\n",
    "\n",
    "Integrated Electromyography (iEMG) is a measure used to quantify the total activation or energy expenditure of a muscle over a specified time interval. It is computed by integrating the absolute amplitude of the electromyography (EMG) signal over time. Here’s how it is typically calculated using the script you provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个用于存储拆分后的DataFrame的列表\n",
    "split_iEMG = []\n",
    "sample_rate = 1000  # 采样率（Hz）\n",
    "x_column = df.columns[0]\n",
    "\n",
    "from scipy.signal import welch\n",
    "\n",
    "\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    abs_emg = split_df[column]\n",
    "\n",
    "    # 计算iEMG\n",
    "    split_iEMG.append(np.trapz(y = np.abs(abs_emg), x = split_df[x_column], dx=0.001))\n",
    "\n",
    "\n",
    "# 打印 iEMG\n",
    "for i, iEMG_df in enumerate(split_iEMG):\n",
    "    print(f\"拆分段 {i + 1} iEMG:\\n{iEMG_df}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='s2m'>3.2.Frequency-field analysis</a>\n",
    "\n",
    "\n",
    "Median Power Frequency (MPF) is a commonly used parameter in the frequency domain analysis of surface electromyography (sEMG) signals. It serves as a crucial metric for assessing muscle fatigue and activation patterns. By identifying the frequency at which half of the signal's power is above and half below, MPF provides insights into muscle recruitment strategies and the spectral characteristics of muscle activity during various tasks. This makes it valuable in fields such as sports science, rehabilitation, and ergonomics, where understanding muscle function and fatigue dynamics is essential for optimizing performance and preventing injury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个用于存储拆分后的DataFrame的列表\n",
    "sample_rate = 1000  # 采样率（Hz）\n",
    "split_MPF = []\n",
    "split_MF = []\n",
    "x_column = df.columns[0]\n",
    "\n",
    "from scipy.signal import welch\n",
    "\n",
    "\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    abs_emg = split_df[column]\n",
    "\n",
    "\n",
    "    # 这里使用welch方法来估计功率谱密度\n",
    "    freq, power_density = welch(abs_emg, fs=sample_rate, nperseg=1024)\n",
    "\n",
    "    # 计算MPF#@\n",
    "    weighted_freq = np.sum(freq * power_density) / np.sum(power_density)\n",
    "    split_MPF.append(weighted_freq)\n",
    "\n",
    "    # 计算MF\n",
    "    MF = freq[np.argmax(np.cumsum(power_density) >= 0.5)]\n",
    "    split_MF.append(MF)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 打印MPF\n",
    "for i, iEMG_df in enumerate(split_MPF):\n",
    "    print(f\"拆分段 {i + 1} iEMG:\\n{iEMG_df}\")\n",
    "\n",
    "# 打印MF\n",
    "for i, iEMG_df in enumerate(split_MF):\n",
    "    print(f\"拆分段 {i + 1} iEMG:\\n{iEMG_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='s2nm'>3.3. Fuzzy entropy</a>\n",
    " calculating Root Mean Square (RMS), normalizing data using MinMaxScaler, and computing Fuzzy Entropy for each segment of the data. Adjustments may be necessary based on your specific data structures and functions (EH.FuzzEn in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each split segment dataframe (example)\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    print(f\"Split Segment {i + 1}:\\n{split_df}\")\n",
    "\n",
    "# Calculate Root Mean Square (RMS)\n",
    "split_rms = []\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    rms = math.sqrt(sum(x ** 2 for x in split_df[column]) / len(split_df))\n",
    "    split_rms.append(rms)\n",
    "\n",
    "# Normalize using MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "split_normal_dfs = []\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    split_normal_dfs.append(scaler_minmax.fit_transform(pd.DataFrame(split_df[column])))\n",
    "\n",
    "# Calculate Fuzzy Entropy\n",
    "split_entropyhub = []\n",
    "r = 0.2\n",
    "n = 2\n",
    "\n",
    "print(f\"{column}\\n\")\n",
    "for i, split_df in enumerate(split_normal_dfs):\n",
    "    raw = split_df[-35000:]  # Adjusted according to your specific needs\n",
    "    th = r * np.std(raw)\n",
    "    Fuzz, Ps1, Ps2 = EH.FuzzEn(raw, m=3, r=(th, n))\n",
    "    split_entropyhub.append(Fuzz[-1])\n",
    "\n",
    "average_entropy = np.mean(split_entropyhub)\n",
    "print(f\"{file_path} + {column} Results:\")\n",
    "print(\"\\nFuzzy Entropy Values:\", split_entropyhub)\n",
    "print(\"Average Fuzzy Entropy:\", average_entropy)\n",
    "print(\"Root Mean Square (RMS):\", split_rms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
